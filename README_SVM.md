# Support Vector Machine (SVM) for Classification
- Hard Margin SVM (Maximum Margin SVM)
å®Œå…¨ä¸å…è¨±èª¤åˆ†é¡
åƒ…é©ç”¨æ–¼ å®Œå…¨ç·šæ€§å¯åˆ† çš„è³‡æ–™
å°‹æ‰¾ä¸€å€‹ä½¿ margin æœ€å¤§åŒ–çš„è¶…å¹³é¢
- Soft Margin SVM (with slack variables)
å®¹è¨±éƒ¨åˆ†æ¨£æœ¬é•å margin æ¢ä»¶
é€é slack variables Î¾iâ€‹ æ§åˆ¶æ¯å€‹æ¨£æœ¬çš„é•åç¨‹åº¦
é©ç”¨æ–¼ ç·šæ€§ä¸å¯åˆ†æˆ–å«å™ªè²è³‡æ–™
- Kernel SVM (using kernel functions for non-linear classification)

##  Introduction
åœ¨æ©Ÿå™¨å­¸ç¿’çš„çœ¾å¤šæ¨¡å‹ä¸­ï¼ŒSVMï¼ˆSupport Vector Machineï¼Œæ”¯æ´å‘é‡æ©Ÿï¼‰ä¸€ç›´æ˜¯åˆ†é¡å•é¡Œä¸­è¡¨ç¾å„ªç•°ã€ä¸”ç†è«–åŸºç¤æ·±åšçš„ç¶“å…¸æ–¹æ³•ä¹‹ä¸€ã€‚æœ¬ç¯‡æ–‡ç« å°‡å¾åŸºç¤ç†è«–è«‡èµ·ï¼Œé€æ­¥ä»‹ç´¹ kernel methodã€SVM çš„æ­·å²ã€æ•¸å­¸æ¨å°ã€å¯¦ä½œæ–¹å¼ã€æ‡‰ç”¨å ´æ™¯ã€èˆ‡å…¶ä»–æ¨¡å‹çš„æ¯”è¼ƒï¼Œä¸¦è£œå……å¯¦å‹™ä¸Šå¸¸è¦‹çš„éŒ¯èª¤èˆ‡é™åˆ¶ï¼Œå¹«åŠ©ä½ å…¨é¢ç†è§£é€™å€‹å¼·å¤§çš„åˆ†é¡å·¥å…·ã€‚

## SVM çš„æ­·å²ç”±ä¾†
SVM çš„ç†è«–ç”± Vladimir Vapnik èˆ‡ Alexey Chervonenkis åœ¨ 1960 å¹´ä»£æå‡ºï¼Œä¸¦æ–¼ 1995 å¹´ç”± Cortes & Vapnik ç™¼è¡¨ç¶“å…¸è«–æ–‡ã€ŠSupport-Vector Networksã€‹æ­£å¼é€²å…¥ä¸»æµæ©Ÿå™¨å­¸ç¿’è¦–é‡ã€‚è‡ªæ­¤ä¹‹å¾Œï¼ŒSVM è¢«å»£æ³›æ‡‰ç”¨æ–¼æ–‡æœ¬åˆ†é¡ã€äººè‡‰è¾¨è­˜ã€å½±åƒè¾¨è­˜ç­‰é ˜åŸŸã€‚

## ä»€éº¼æ˜¯ Kernel Methodï¼Ÿ
Kernel æ–¹æ³•æ˜¯ä¸€ç¨®é€ééç·šæ€§æ˜ å°„å°‡ä½ç¶­ç‰¹å¾µç©ºé–“è³‡æ–™æŠ•å½±è‡³é«˜ç¶­ç©ºé–“ï¼Œä½¿å¾—åŸæœ¬ç„¡æ³•ç·šæ€§åˆ†é›¢çš„è³‡æ–™åœ¨é«˜ç¶­ç©ºé–“ä¸­è®Šå¾—å¯åˆ†ã€‚é€™å€‹éç¨‹ä¸éœ€å¯¦éš›è¨ˆç®—é«˜ç¶­ç©ºé–“åº§æ¨™ï¼Œè€Œæ˜¯é€é kernel function ä¾†é–“æ¥è¨ˆç®—å…§ç©ã€‚

å¸¸è¦‹çš„ kernel æœ‰ï¼š
ç·šæ€§æ ¸ï¼ˆLinear Kernelï¼‰ï¼š( K(x, x') = x^T x' \ï¼‰
å¤šé …å¼æ ¸ï¼ˆPolynomial Kernelï¼‰ï¼š( K(x, x') = (x^T x' + c)^d \ï¼‰
RBF æ ¸ï¼ˆé«˜æ–¯å¾‘å‘åŸºï¼‰ï¼š( K(x, x') = \exp(-\gamma ||x - x'||^2) \ï¼‰
Sigmoid æ ¸ï¼š( \tanh(\alpha x^T x' + c) \ï¼‰
Kernel æ–¹æ³•æ˜¯ SVM èƒ½å¤ æ‡‰ä»˜éç·šæ€§åˆ†é¡å•é¡Œçš„æ ¸å¿ƒæŠ€å·§ã€‚

SVM çµåˆ Kernel æŠ€è¡“å¾Œï¼Œèƒ½å¤ åœ¨é«˜ç¶­ç©ºé–“ä¸­å°‹æ‰¾æœ€å¤§é–“éš”ï¼ˆMarginï¼‰çš„è¶…å¹³é¢ï¼Œä¾†é€²è¡Œåˆ†é¡ã€‚é€™ä½¿å¾—å®ƒåœ¨é«˜ç¶­ã€éç·šæ€§çš„æƒ…å¢ƒä¸‹ä¾ç„¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

## æ•¸å­¸ç†è«–èˆ‡å…¬å¼æ¨å°
SVM çš„ç›®æ¨™æ˜¯æœ€å¤§åŒ–åˆ†é¡é‚Šç•Œçš„ marginï¼ŒåŒæ™‚å®¹å¿å°‘é‡èª¤åˆ†é¡ã€‚å…¶åŸºæœ¬å½¢å¼å¦‚ä¸‹ï¼š
![image](./resource/svm/05.png)

å…¶ä¸­ï¼š
wï¼šè¶…å¹³é¢æ³•å‘é‡
bï¼šåå·®é …
ğœ‰ğ‘–ï¼šslack variableï¼Œå…è¨±éƒ¨åˆ†æ¨£æœ¬é•å margin æ¢ä»¶
Cï¼šæ‡²ç½°é•è¦æ¨£æœ¬çš„ç¨‹åº¦ï¼ˆregularization parameterï¼‰

åœ¨ dual form ä¸­ï¼Œæœƒå‡ºç¾ Lagrangian multiplier èˆ‡ kernel functionï¼Œæ–¹ä¾¿æ‡‰ç”¨éç·šæ€§æ˜ å°„ã€‚

-----------------------------------
ç¯„ä¾‹å‡ç‚ºä»¥ä¸‹è³‡æ–™ä¾†è¬›è§£ï¼š
ğ‘¤ = [âˆ’ 1 / 4, 1 / 4], b = - 1 /4
X = np.array([[1,8],[7,2],[6,-1],[-5,0], [-5,1], [-5,2],[6,3],[6,1],[5,2]])
y = np.array([1, -1, -1, 1, -1, 1, 1, -1, -1])

## Definitions
- decision boundary
Decision Boundary æ˜¯ SVM ç”¨ä¾†å€åˆ†ä¸åŒé¡åˆ¥çš„é‚Šç•Œã€‚å®ƒæ˜¯ç”±è¶…å¹³é¢ï¼ˆhyperplaneï¼‰å®šç¾©çš„ï¼Œä¸¦ä¸”å°‡è³‡æ–™é»åˆ†ç‚ºå…©å€‹é¡åˆ¥ã€‚å°æ–¼äºŒå…ƒåˆ†é¡å•é¡Œï¼Œæ±ºç­–é‚Šç•Œå¯ä»¥ç”¨ä»¥ä¸‹æ–¹ç¨‹å¼è¡¨ç¤ºï¼š
  - ğ‘¤â‹…ğ‘¥ + ğ‘ = 0
  å…¶ä¸­ï¼Œğ‘¤ æ˜¯æ¬Šé‡å‘é‡ï¼Œğ‘¥ æ˜¯è¼¸å…¥ç‰¹å¾µå‘é‡ï¼Œğ‘ æ˜¯åç½®é …ã€‚
- hyperplane
  - åœ¨äºŒç¶­ç©ºé–“ä¸­ï¼Œè¶…å¹³é¢å°±æ˜¯ä¸€æ¢ç›´ç·šã€‚
  - åœ¨ä¸‰ç¶­ç©ºé–“ä¸­ï¼Œè¶…å¹³é¢æ˜¯ä¸€å€‹å¹³é¢ã€‚
  - åœ¨æ›´é«˜ç¶­åº¦ä¸­ï¼Œè¶…å¹³é¢æ˜¯ n-1 ç¶­çš„å­ç©ºé–“ã€‚
  - è¶…å¹³é¢å¯ä»¥ç”¨ä»¥ä¸‹æ–¹ç¨‹å¼è¡¨ç¤ºï¼š
    ğ‘¤â‹…ğ‘¥ + ğ‘ = 0
    å…¶ä¸­ï¼Œğ‘¤ æ˜¯æ¬Šé‡å‘é‡ï¼Œğ‘¥ æ˜¯è¼¸å…¥ç‰¹å¾µå‘é‡ï¼Œğ‘ æ˜¯åç½®é …ã€‚
- support vectors
Support Vectors æ˜¯å‰›å¥½è½åœ¨ margin ä¸Šçš„è³‡æ–™é»ï¼Œä¹Ÿå°±æ˜¯é‚£äº›æ»¿è¶³ä»¥ä¸‹æ¢ä»¶çš„æ¨£æœ¬ï¼š
  - ğ‘¦ğ‘–(ğ‘¤â‹…ğ‘¥ğ‘– + ğ‘) = 1 ï¼ˆåœ¨Â marginÂ é‚Šç•Œä¸Šï¼‰
æˆ–è€…åœ¨ soft-margin SVM ä¸­ï¼Œä¹ŸåŒ…å«ï¼š
  - ğ‘¦ğ‘–(ğ‘¤â‹…ğ‘¥ğ‘– + ğ‘) = 1 - ğœ‰ğ‘– ï¼ˆåœ¨Â marginÂ å…§éƒ¨ï¼Œä¸” slack è®Šæ•¸ ğœ‰ğ‘– > 0ï¼‰
![image](./resource/svm/03.png)    

  - out-boundary support vectors: 
    - ğ‘¦ğ‘–(ğ‘¤â‹…ğ‘¥ğ‘– + ğ‘) = 1 ï¼ˆåœ¨ margin é‚Šç•Œä¸Šï¼‰(ğœ‰ğ‘– â‰¥ 1)
    - ğœ‰ğ‘– = 0 ï¼ˆåœ¨ margin å¤–å´ï¼ŒéŒ¯èª¤åˆ†é¡ï¼‰
  - in-boundary support vectors:
    - ğ‘¦ğ‘–(ğ‘¤â‹…ğ‘¥ğ‘– + ğ‘) = 1 - ğœ‰ğ‘– ï¼ˆåœ¨ margin å…§éƒ¨ï¼Œä¸” slack è®Šæ•¸ ğœ‰ğ‘– > 0ï¼‰
    - ğœ‰ğ‘– = 0 æˆ–æ˜¯ 0 < ğœ‰ğ‘– < 1 ï¼ˆåœ¨ margin è£¡ï¼Œæˆ–æ˜¯åœ¨marginé‡Œå¯æ˜¯æ­£ç¢ºåˆ†é¡ï¼‰
Once you fit your SVM model, you can access:
``` python
nlsvm.support_vectors_         # All support vector coordinates
nlsvm.dual_coef_               # Dual coefficients (Î±_i * y_i), shape (1, n_support)
nlsvm.C                        # The regularization parameter C
```

To Classify:
``` python
import numpy as np

# Get indices of all support vectors
support_indices = nlsvm.support_

# Dual coefficients (signed)
dual_coefs = np.abs(nlsvm.dual_coef_[0])

# Support vectors at bound (alpha == C)
out_bound_mask = np.isclose(dual_coefs, nlsvm.C)
in_bound_mask = ~out_bound_mask

# Coordinates
in_bound_SVs = nlsvm.support_vectors_[in_bound_mask]
out_bound_SVs = nlsvm.support_vectors_[out_bound_mask]

print("In-bound Support Vectors:\n", in_bound_SVs)
print("Out-bound Support Vectors:\n", out_bound_SVs)
```

- margin boundaries
Margin= 2 / âˆ¥wâˆ¥
â€‹âˆ¥wâˆ¥ : norm of the weight vector w, L2 Euclidean norm
w is the weight vector perpendicular to the hyperplane (è¶…å¹³é¢çš„æ³•å‘é‡)
For example: 
è¶…å¹³é¢æ³•å‘é‡ 
ğ‘¤ = [âˆ’ 1 / 4, 1 / 4], b = - 1 /4
âˆ¥wâˆ¥ = âˆš((-1/4)Â² + (1/4)Â²) = âˆš(1/16 + 1/16) = âˆš(2/16) = âˆš(1/8) = 1 / 2âˆš2
margin = 2 / âˆ¥wâˆ¥ = 2 / (1 / 2âˆš2) = 4âˆš2
â€‹
- nonzero slack é¬†å¼›è®Šé‡
æ˜¯æŒ‡æŸå€‹è¨“ç·´æ¨£æœ¬é•åäº† margin çš„æ¢ä»¶ï¼Œä¹Ÿå°±æ˜¯èªªï¼š
Slack è®Šæ•¸ï¼ˆè¨˜ä½œ ğœ‰ğ‘–ï¼‰æ˜¯åœ¨ è»Ÿé–“éš”ï¼ˆsoft marginï¼‰SVM ä¸­å¼•å…¥çš„ï¼Œç‚ºäº†å…è¨±æ¨¡å‹å°æ–¼ä¸å®Œç¾å¯åˆ†çš„è³‡æ–™ä»èƒ½å»ºç«‹åˆ†é¡é‚Šç•Œã€‚

æ¯ä¸€ç­†è³‡æ–™éƒ½æœ‰ä¸€å€‹ slack è®Šæ•¸ ğœ‰ğ‘–ï¼Œå…¶å«ç¾©å¦‚ä¸‹ï¼š
ğœ‰ğ‘– = 0ï¼šè©²é»å®Œå…¨æ»¿è¶³ SVM çš„ margin æ¢ä»¶ï¼Œè€Œä¸”åœ¨é‚Šç•Œä¹‹å¤–ã€‚
0 < ğœ‰ğ‘– < 1ï¼šè©²é»åœ¨ margin ä¹‹å…§ï¼Œä½†åˆ†é¡ä»æ˜¯æ­£ç¢ºçš„ã€‚
ğœ‰ğ‘– â‰¥ 1ï¼šè©²é»è¢«éŒ¯èª¤åˆ†é¡ï¼ˆmisclassifiedï¼‰
å°æ¯ä¸€ç­†è³‡æ–™é» (ğ‘¥ğ‘–, ğ‘¦ğ‘–)ï¼Œç´„æŸæ¢ä»¶ç‚ºï¼š
    ğ‘¦ğ‘–(ğ‘¤â‹…ğ‘¥ğ‘– + ğ‘) â‰¥ 1 - ğœ‰ğ‘–, ğœ‰ğ‘– â‰¥ 0
=>  ğœ‰ğ‘– = max(0, 1 - ğ‘¦ğ‘–(ğ‘¤â‹…ğ‘¥ğ‘– + ğ‘))
ç™½è©±æ–‡ï¼š
ç•¶ ğœ‰ğ‘– > 0, 
- é€™ç­†è³‡æ–™ï¼šæ²’æœ‰åœ¨ margin å¤–å´ï¼ˆä¸æ˜¯ç†æƒ³ support vectorï¼‰
- æœ‰å…©ç¨®å¯èƒ½æƒ…æ³ï¼š
0 < ğœ‰ğ‘– < 1ï¼šåœ¨ margin è£¡ï¼Œä½†æ²’è¶Šç•Œ
ğœ‰ğ‘– â‰¥ 1ï¼šå®Œå…¨è¢«åˆ†éŒ¯é‚Š â†’ éŒ¯èª¤åˆ†é¡
=> Nonzero slack å°±æ˜¯æŸç­†è³‡æ–™çš„ slack å€¼ 
ğœ‰ğ‘– > 0ï¼Œä»£è¡¨é€™ç­†è³‡æ–™è½åœ¨ margin è£¡é¢ç”šè‡³è¢«åˆ†é¡éŒ¯èª¤ï¼Œæ¨¡å‹ç‚ºäº†æ•´é«”æœ€ä½³åˆ†é¡è¡¨ç¾è€Œã€ŒçŠ§ç‰²ã€å®ƒã€‚
------------------------------------
For each data point (ğ‘¥ğ‘–, ğ‘¦ğ‘–),  the constraint is:
    ğ‘¦ğ‘–(ğ‘¤â‹…ğ‘¥ğ‘– + ğ‘) â‰¥ 1 - ğœ‰ğ‘–, 
    where ğœ‰ğ‘– â‰¥ 0 is the slack variable for the i-th data point.
    if ğœ‰ğ‘– > 0, the point is either: 
    - inside the margin (but correctly classified)
    - misclassified
So, to find training points with nonzero slack, we compute for each point:
    margin condition = ğ‘¦ğ‘–(ğ‘¤â‹…ğ‘¥ğ‘– + ğ‘) < 1
If the margin condition is true, then ğœ‰ğ‘– > 0, meaning the point has nonzero slack.

``` python 
# ğ‘¤ = [âˆ’ 1 / 4, 1 / 4], b = - 1 /4
X = np.array([[1,8],[7,2],[6,-1],[-5,0], [-5,1], [-5,2],[6,3],[6,1],[5,2]])
y = np.array([1, -1, -1, 1, -1, 1, 1, -1, -1])
```
![image](./resource/svm/01.png)

Slack variable formula
For each training point (ğ‘¥ğ‘–, ğ‘¦ğ‘–):
    ğœ‰ğ‘– = max(0, 1 - ğ‘¦ğ‘–(ğ‘¤â‹…ğ‘¥ğ‘– + ğ‘))

![image](./resource/svm/02.png)

- soft margin   
åœ¨ Soft-Margin ä¸­ï¼Œæå¤±å‡½æ•¸èˆ‡ slack variable æ˜¯é€™æ¨£çš„é—œä¿‚ï¼š
- soft margin loss function
  - L = 1/2 ||w||Â² + C * Î£ğœ‰ğ‘–
  - L(x, y) = max(0, 1 - y * (wâ‹…x + b)) + C * Î£ğœ‰ğ‘– => ğœ‰ğ‘– = L(x, y)
  - å…¶ä¸­ï¼ŒC æ˜¯æ­£å‰‡åŒ–åƒæ•¸ï¼Œæ§åˆ¶ slack çš„æ‡²ç½°ç¨‹åº¦ã€‚
  - é€™å€‹æå¤±å‡½æ•¸çš„ç›®æ¨™æ˜¯æœ€å°åŒ–æ¬Šé‡å‘é‡çš„å¹³æ–¹å’Œï¼ŒåŒæ™‚å° slack è®Šæ•¸é€²è¡Œæ‡²ç½°ï¼Œä»¥å…è¨±éƒ¨åˆ†è³‡æ–™é»é•å margin æ¢ä»¶ã€‚
  å› æ­¤ï¼Œhinge loss æœ¬è³ªä¸Šå°±æ˜¯ slack çš„ä¸€ç¨®è¡¡é‡ã€‚
- kernel trick
- kernel function
- loss function
  - å¯¦å‹™å»ºè­°
    å¦‚æœä½ çš„è³‡æ–™ä¸­æœ‰äº›é›œè¨Šæˆ–é›¢ç¾¤å€¼ â†’ ä½¿ç”¨ hinge lossï¼ˆæ›´ç©©å®šï¼‰
    å¦‚æœä½ çš„è³‡æ–™å¾ˆä¹¾æ·¨ã€ä¸”ä½ æƒ³è¦æ¨¡å‹æ›´åš´æ ¼ â†’ å¯ä»¥è€ƒæ…® squared hinge loss
  - hinge loss
  å¦‚æœä¸€ç­†è³‡æ–™é»è·é›¢æ±ºç­–é‚Šç•Œè¶…é marginï¼ˆä¹Ÿå°±æ˜¯ yâ‹…f(x)â‰¥1ï¼‰ï¼Œé‚£éº¼ loss ç‚º 0ã€‚
  å¦å‰‡ï¼Œæå¤±æœƒç·šæ€§å¢åŠ ï¼Œè·é›¢é‚Šç•Œè¶Šè¿‘æˆ–åˆ†é¡éŒ¯èª¤ï¼Œæå¤±è¶Šå¤§ã€‚
  å°æ–¼ é›¢ç¾¤é»ï¼ˆoutliersï¼‰ çš„æ‡²ç½°è¼ƒæº«å’Œã€‚
  å¸¸å¸¸æœƒç”¢ç”Ÿè¼ƒç¨€ç–ï¼ˆsparseï¼‰ çš„æ¨¡å‹ï¼ˆsupport vector è¼ƒå°‘ï¼‰ã€‚
    - Hinge loss is a loss function used in SVMs to maximize the margin between classes.
    - It penalizes misclassified points and those within the margin.
    - The hinge loss for a single point (ğ‘¥ğ‘–, ğ‘¦ğ‘–) is defined as:
      - max(0, 1 - ğ‘¦ğ‘– * (ğ‘¤â‹…ğ‘¥ğ‘– + ğ‘))
    - This means:
      - If the point is correctly classified and outside the margin, the loss is 0.
      - If it is misclassified or within the margin, the loss increases linearly.
  - Squared hinge loss(p = 2)
  èˆ‡ hinge loss é¡ä¼¼ï¼Œä½†æŠŠæå¤±å¹³æ–¹äº†ã€‚
  å¦‚æœä¸€ç­†è³‡æ–™é»è·é›¢é‚Šç•Œä¸å¤ é ï¼Œæå¤±æœƒä»¥å¹³æ–¹æ–¹å¼æˆé•·ã€‚
  å°æ–¼åˆ†é¡éŒ¯èª¤çš„è³‡æ–™ï¼ˆå°¤å…¶é›¢ç¾¤é»ï¼‰ï¼Œæ‡²ç½°æ›´å¤§ã€‚
  æ¨¡å‹æœƒå˜—è©¦æ›´åš´æ ¼åœ°å€åˆ†è³‡æ–™ï¼Œä½†ä¹Ÿå¯èƒ½å› æ­¤éæ“¬åˆï¼ˆoverfitï¼‰ã€‚
    - Squared hinge loss is a variant of hinge loss that squares the margin violation.
    - It is defined as:
      - max(0, 1 - ğ‘¦ğ‘– * (ğ‘¤â‹…ğ‘¥ğ‘– + ğ‘))Â²
    - This means:
      - It penalizes misclassified points more heavily than hinge loss, as the penalty grows quadratically with the margin violation.
  - Practical Observations
  ![image](./resource/svm/04.png)
    - Hinge loss is less sensitive to outliers than squared hinge loss.
    - Squared hinge loss can lead to larger gradients for misclassified points, potentially speeding up convergence but also increasing sensitivity to outliers.

## How do C and gamma affect the decision boundary?
C æ§åˆ¶æ¨¡å‹å°éŒ¯èª¤åˆ†é¡çš„å¯¬å®¹åº¦ï¼Œgamma æ§åˆ¶ kernel å°è³‡æ–™é»çš„ã€Œå½±éŸ¿ç¯„åœã€èˆ‡é‚Šç•Œå½æ›²ç¨‹åº¦ã€‚
å…©è€…éœ€ä¸€èµ·èª¿æ•´ï¼Œæ±ºå®šæ¨¡å‹æ˜¯ä¿å®ˆã€æ³›åŒ–é‚„æ˜¯è¨˜æ†¶æ¯ä¸€é»ã€‚

C å®¹éŒ¯æ‡²ç½°åƒæ•¸ï¼ˆRegularization parameterï¼‰: 
åŠŸèƒ½ï¼š
æ§åˆ¶æ¨¡å‹å°ã€ŒéŒ¯èª¤åˆ†é¡ã€çš„å®¹å¿ç¨‹åº¦ã€‚
![image](./resource/svm/12.png)
![image](./resource/svm/13.png)
Regularization strength
- Low C â†’ more tolerant of misclassified points (smoother, more generalized boundary)
- High C â†’ less tolerance â†’ more complex boundary, fits training set better

gamma RBF Kernel çš„å½æ›²ç¨‹åº¦åƒæ•¸ :
åŠŸèƒ½ï¼š
æ§åˆ¶æ¯å€‹è¨“ç·´é»çš„ã€Œå½±éŸ¿ç¯„åœã€ï¼Œä¹Ÿå°±æ˜¯Kernel çš„æ„ŸçŸ¥ç¯„åœï¼ˆç¯„åœå¤§æˆ–å°ï¼‰
![image](./resource/svm/14.png)
![image](./resource/svm/15.png)
RBF kernel width (inverse of std)	
- Low gamma â†’ broader influence of each point (smoother boundary)
- High gamma â†’ each point only influences nearby region â†’ more wiggly, overfit-prone boundary

## Linear SVM
### Full code example
```python
from sklearn.svm import LinearSVC
import numpy as np

# Given data
X = np.array([[1, 8], [7, 2], [6, -1], [-5, 0], [-5, 1], [-5, 2], [6, 3], [6, 1], [5, 2]])
y = np.array([1, -1, -1, 1, -1, 1, 1, -1, -1])

# Convert labels to +1 / -1 â†’ LinearSVC requires {0,1}, so we map to {0,1}
y_transformed = (y == 1).astype(int)

# Train LinearSVC model with hinge loss (p=1) and C=1
lsvm = LinearSVC(C=1, loss='hinge', max_iter=10000)
lsvm.fit(X, y_transformed)

# Get model parameters
w = lsvm.coef_[0]
b = lsvm.intercept_[0]
print("Weight vector:", w)
print("Bias:", b)

# Plot using the provided utility (assumes this function exists)
linear_plot(X, y, w=w, b=b)
```

## Kernel æ–¹æ³•çš„å¹¾ç¨®å¸¸è¦‹å½¢å¼èˆ‡æ¯”è¼ƒ
ç·šæ€§ï¼ˆLinearï¼‰
å¤šé …å¼ï¼ˆPolynomialï¼‰
é«˜æ–¯/å¾‘å‘åŸºï¼ˆRBF / Gaussianï¼‰
Sigmoid
ä½¿ç”¨è‡ªå®šç¾© kernel çš„å¯¦ä¾‹

## SVM è¨“ç·´çš„æ•¸å€¼å„ªåŒ–æ–¹æ³•
æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ï¼ˆLagrangian dualityï¼‰
SMOï¼ˆSequential Minimal Optimizationï¼‰æ¦‚å¿µ
primal vs. dual å•é¡Œçš„è§£æ³•

## è™•ç†å¤šåˆ†é¡å•é¡Œçš„ç­–ç•¥
è‡‰éƒ¨è¾¨è­˜ï¼ˆå¦‚ Eigenfaces æ­é… SVMï¼‰
æ‰‹å¯«æ•¸å­—è¾¨è­˜ï¼ˆMNISTï¼‰
ç‰©ä»¶åµæ¸¬ï¼ˆå¦‚åœ¨ç‰¹å¾µç©ºé–“ä½¿ç”¨ HOG + SVMï¼‰
ç—…ç†å½±åƒåˆ†é¡ï¼ˆé†«å­¸å½±åƒä¸­çš„åˆ†é¡æ‡‰ç”¨ï¼‰
åƒåœ¾éƒµä»¶åˆ†é¡

## è©•ä¼°æŒ‡æ¨™èˆ‡æ¨¡å‹å„ªåŒ–
å¸¸è¦‹çš„è©•ä¼°æŒ‡æ¨™åŒ…æ‹¬ï¼š
Accuracy
Precision / Recall / F1-score
Confusion Matrix
ROC Curve / AUC
å¯å„ªåŒ–æ–¹å‘ï¼š
èª¿æ•´ C èˆ‡ gamma
ç‰¹å¾µæ¨™æº–åŒ–ï¼ˆscalingï¼‰
Kernel é¸æ“‡èˆ‡åƒæ•¸èª¿æ•´
ä½¿ç”¨å­¸ç¿’æ›²ç·šåˆ†æ bias/variance tradeoff underfitting / overfitting

## æ¨¡å‹æ•ˆèƒ½è©•ä¼°æŠ€å·§è£œå……
æ··æ·†çŸ©é™£ï¼ˆConfusion Matrixï¼‰
ROC Curve èˆ‡ AUCï¼ˆé‡å°äºŒå…ƒåˆ†é¡ï¼‰
Precision/Recall Tradeoff
æ¨¡å‹å­¸ç¿’æ›²ç·šã€åå·®-è®Šç•°åˆ†æï¼ˆbias-variance tradeoffï¼‰

## å»¶ä¼¸ä¸»é¡Œ
SVRï¼ˆSupport Vector Regressionï¼‰
One-Class SVMï¼ˆç•°å¸¸åµæ¸¬ï¼‰
SVM + PCA é™ç¶­æ•´åˆ
ä½¿ç”¨ GPU åŠ é€Ÿçš„ SVM å¯¦ä½œï¼ˆå¦‚ cuMLï¼‰

## SVM çš„å„ªé»ï¼ˆStrengthsï¼‰
![image](./resource/svm/07.png)

## SVM çš„ç¼ºé»ï¼ˆLimitationsï¼‰
![image](./resource/svm/08.png)

## SVM é©åˆçš„è³‡æ–™é›†ç‰¹æ€§
![image](./resource/svm/09.png)

## ä¸å»ºè­°ä½¿ç”¨ SVM çš„å ´æ™¯
![image](./resource/svm/10.png)
![image](./resource/svm/11.png)

## èˆ‡å…¶ä»–æ¨¡å‹æ¯”è¼ƒ
![image](./resource/svm/06.png)

## é©åˆä½¿ç”¨ SVMÂ çš„æƒ…å¢ƒ
ç‰¹å¾µç¶­åº¦é«˜ã€æ¨£æœ¬æ•¸ä¸­ç­‰æˆ–åå°‘çš„ä»»å‹™
Â ä¾‹ï¼šåŸºå› è¡¨é”åˆ†æã€æ–‡å­—åˆ†é¡ã€HOG ç‰¹å¾µåˆ†é¡
éœ€è¦åš´è¬¹é‚Šç•Œèˆ‡æœ€å¤§åŒ–åˆ†é¡é–“è·çš„ä»»å‹™
Â ä¾‹ï¼šç•°å¸¸åµæ¸¬ã€ç‘•ç–µæª¢æ¸¬ã€å®‰å…¨åˆ†é¡ç³»çµ±
è³‡æ–™å…·æœ‰ä¸€å®šå¯åˆ†æ€§ï¼Œæˆ–èƒ½è—‰ç”± kernel æ˜ å°„è®Šå¯åˆ†
Â é©åˆä½¿ç”¨ç·šæ€§æˆ–éç·šæ€§ SVMï¼ˆRBFã€Polynomialï¼‰
å¸Œæœ›æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¼·ã€é¿å…éåº¦æ“¬åˆæ™‚
Â é©ç”¨æ–¼ C èˆ‡ kernel å¯èª¿æ•´çš„æƒ…å¢ƒ
åœ¨å°æ¨£æœ¬æƒ…æ³ä¸‹ä»éœ€ç©©å®šåˆ†é¡æ•ˆæœæ™‚
Â ç›¸è¼ƒæ–¼æ·±åº¦å­¸ç¿’ï¼ŒSVM åœ¨è³‡æ–™ç¨€å°‘æ™‚æ›´ç©©å®š

## ä¸å»ºè­°ä½¿ç”¨ SVMÂ çš„æƒ…å¢ƒ
æ¨£æœ¬æ•¸éå¸¸å¤§ï¼ˆ>10 è¬ç­†ä»¥ä¸Šï¼‰æ™‚
Â è¨“ç·´é€Ÿåº¦èˆ‡è¨˜æ†¶é«”éœ€æ±‚æœƒé¡¯è‘—ä¸Šå‡
ç‰¹å¾µæ•¸é‡æ¥µå°‘ã€è³‡æ–™è¦å‰‡ç°¡å–®å¯åˆ†æ™‚
Â å¯è€ƒæ…®ç”¨ Logistic Regressionã€Decision Tree æ›´å¿«é€Ÿå¯è§£é‡‹
éœ€è¦æ¨¡å‹é«˜åº¦å¯è§£é‡‹æ€§æˆ–æ˜“æ–¼å‘ˆç¾æ±ºç­–é‚è¼¯æ™‚
Â SVM çš„æ±ºç­–é‚Šç•Œé›£ä»¥ç›´æ¥ç†è§£èˆ‡è¦–è¦ºåŒ–
è³‡æ–™æ¨™è¨»å“è³ªä¸é«˜æˆ–å«å¤§é‡å™ªè²ã€é›¢ç¾¤å€¼æ™‚
Â SVMï¼ˆç‰¹åˆ¥æ˜¯ hinge lossï¼‰å°é€™é¡è³‡æ–™è¼ƒç‚ºæ•æ„Ÿ
å¤šé¡åˆ¥æ•¸é‡éå¤šæ™‚ï¼ˆä¾‹å¦‚æ•¸åé¡ä»¥ä¸Šï¼‰
Â é›–ç„¶å¯ä»¥ç”¨ One-vs-One æˆ– One-vs-Restï¼Œä½†è¨“ç·´èˆ‡é æ¸¬è¤‡é›œåº¦æœƒå¿«é€Ÿä¸Šå‡

## å¸¸è¦‹éŒ¯èª¤èˆ‡é™åˆ¶
æœªæ¨™æº–åŒ–ç‰¹å¾µï¼ˆæœƒå½±éŸ¿ kernel è¨ˆç®—ï¼‰
gamma éå¤§å°è‡´ overfitting
C éå°å°è‡´ underfitting
è³‡æ–™é›†éå¤§æ™‚ï¼ŒSVM è¨“ç·´é€Ÿåº¦æ…¢ï¼ˆè€ƒæ…®ä½¿ç”¨è¿‘ä¼¼æ–¹æ³•æˆ–è½‰å‘å…¶ä»–æ¨¡å‹ï¼‰

## è³‡æºèˆ‡å­¸è¡“è³‡æ–™æ¨è–¦
ğŸ“˜ æ›¸ç±ï¼š
ã€ŠPattern Recognition and Machine Learningã€‹by Christopher Bishop
ã€ŠUnderstanding Machine Learningã€‹by Shai Shalev-Shwartz and Shai Ben-Davidï¼ˆæœ‰è©³ç´°æ¨å° SVM ç†è«–ï¼‰
ã€ŠGuide to Kernel-based Learning Algorithmsã€‹

ğŸ“„ ç¶“å…¸è«–æ–‡ï¼š
Cortes & Vapnik (1995): Support-Vector Networks
Dalal & Triggs (2005): Histograms of Oriented Gradients for Human Detection
Osuna, Freund & Girosi: An Improved Training Algorithm for SVMs
Scholkopfâ€™s papers on Kernel PCA èˆ‡ One-Class SVM

## References
- [Slack](https://ljalphabeta.gitbooks.io/python-/content/slack.html)
- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
- [Support Vector Machines (SVM): An Intuitive Explanation](https://medium.com/low-code-for-advanced-data-science/support-vector-machines-svm-an-intuitive-explanation-b084d6238106)
- [Recognizing hand-written digits](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)
- [What are support vector machines (SVMs) IBM Article](https://www.ibm.com/think/topics/support-vector-machine)